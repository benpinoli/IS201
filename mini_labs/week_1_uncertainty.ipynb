{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoCSNT_XRFIE"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/mini_labs/week_1_uncertainty.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "# BYU CS 473 — Types of Uncertainty\n",
        "\n",
        "In this assignment, you will explore **epistemic** and **aleatoric** uncertainty in machine learning.\n",
        "You will use a small dataset to visualize predictions, compute uncertainty, and reflect on the differences between these two key concepts.\n",
        "\n",
        "---\n",
        "## Learning Goals\n",
        "- Understand the difference between **epistemic** (knowledge/model) and **aleatoric** (data/noise) uncertainty\n",
        "- Learn how to model uncertainty using probability distributions\n",
        "- Implement uncertainty estimation with a softmax-based classifier\n",
        "- Visualize how uncertainty appears in decision boundaries\n",
        "- Practice interpreting results and reflecting on uncertainty in real-world ML tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9nM0PjqQMNI"
      },
      "source": [
        "## Part 1 — dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo3GJXqnQKOg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=400, n_features=2, n_redundant=0, n_clusters_per_class=1,\n",
        "    class_sep=1.0, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91cze2w0QXq_"
      },
      "source": [
        "## Part 2 — Softmax Probabilities (Aleatoric Uncertainty)\n",
        "\n",
        "Here you’ll train a logistic regression classifier and use its predicted probabilities to measure **aleatoric uncertainty**.\n",
        "Aleatoric uncertainty shows up when points are near the decision boundary (probabilities close to 0.5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH5_Rw9oQlDv"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "\n",
        "probs = clf.predict_proba(X_test)\n",
        "preds = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj5q2i5Ex-21"
      },
      "source": [
        "### TODO:\n",
        "\n",
        "Find the points that have the most uncertain predictions.  Think about this a little bit - can you come up with a way to quantify a \"certain prediction\" vs. an \"uncertain prediction\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exEDl2JZx-22"
      },
      "outputs": [],
      "source": [
        "# XXX your code here\n",
        "\n",
        "# Show most uncertain predictions\n",
        "most_uncertain_idx = np.argsort(-uncertainty)[:5]\n",
        "print(\"Most uncertain samples (indices):\", most_uncertain_idx)\n",
        "print(\"Probabilities for those samples:\\n\", probs[most_uncertain_idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PHjpcGcx-22"
      },
      "source": [
        "## Part 3 — Visualizing Uncertainty\n",
        "\n",
        "---\n",
        "\n",
        "### What is a decision boundary?  \n",
        "A **decision boundary** is a line that a classifier learns to **separate different classes** of data.  \n",
        "- Points on one side of the boundary are predicted as one class.  \n",
        "- Points on the other side are predicted as another.  \n",
        "\n",
        "In two dimensions, we can plot this boundary directly. The shaded regions in the visualization show the classifier’s predicted classes, while the dots are the actual data points.  \n",
        "\n",
        "Where the dots overlap or cluster near the boundary, the classifier has more difficulty deciding — this is where we see **uncertainty**.  \n",
        "\n",
        "- **Aleatoric uncertainty**: appears near the decision boundary, caused by noisy or overlapping data.  \n",
        "- **Epistemic uncertainty**: occurs in regions far away from any training data (where the model is unsure because it has not “seen” those inputs before).  \n",
        "\n",
        "---\n",
        "\n",
        "When you look at the plot:  \n",
        "- The **shaded regions** represent the classifier’s predictions.  \n",
        "- The **solid boundary line** shows where the model is exactly 50/50 between classes.  \n",
        "- The **data points** show the ground-truth labels.  \n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ru-2N7x-22"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(clf, X, y, title=\"Decision boundary\"):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(clf, X_train, y_train, \"Aleatoric uncertainty example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuAGG6AmRJ8u"
      },
      "source": [
        "## Part 4 — Epistemic Uncertainty (via Model Variation)\n",
        "\n",
        "Epistemic uncertainty can be explored by varying the model or subsampling the training data.\n",
        "We’ll train multiple logistic regression models on bootstrapped samples and measure the variation in predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZB4ti7URO03"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "n_models = 20\n",
        "all_probs = []\n",
        "\n",
        "for i in range(n_models):\n",
        "    X_boot, y_boot = resample(X_train, y_train, replace=True, random_state=i)\n",
        "    clf_boot = LogisticRegression().fit(X_boot, y_boot)\n",
        "    all_probs.append(clf_boot.predict_proba(X_test))\n",
        "\n",
        "all_probs = np.array(all_probs)  # shape (n_models, n_samples, n_classes)\n",
        "\n",
        "# Epistemic uncertainty = variance across models\n",
        "epistemic_uncertainty = all_probs.var(axis=0).mean(axis=1)\n",
        "\n",
        "print(\"Sample epistemic uncertainties:\", epistemic_uncertainty[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFl6dHetx-23"
      },
      "source": [
        "### TODO\n",
        "\n",
        "Now, using the plot_decision_boundary function from Part 3 and the code in Part 4, can you come up with a way to visualize the uncertainty in the decision boundary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZccnxxyVx-24"
      },
      "outputs": [],
      "source": [
        " # XXX your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRgQ_tGuRhpD"
      },
      "source": [
        "## Part 5 — Reflection\n",
        "\n",
        "Answer the following in 2–3 sentences each:\n",
        "\n",
        "1. Which type of uncertainty did you observe near the decision boundary? Why?  \n",
        "2. How did epistemic uncertainty change across different bootstrap models?  \n",
        "3. Give a real-world example where epistemic uncertainty dominates, and one where aleatoric uncertainty dominates.  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}